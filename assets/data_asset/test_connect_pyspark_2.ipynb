{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1a171ea8-433e-4ccd-bfcf-2aa1a5eb02ea"
   },
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql  import SparkSession \n",
    "from pyspark.sql.functions import collect_list, col, lit \n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, TimestampType, DateType, ArrayType\n",
    "import os \n",
    "import json \n",
    "from pyspark.sql.functions import to_date, when\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "d9851f19-f355-4279-a586-4e90780b3f46",
    "msg_id": "6999d6ca-44f0-4002-ad01-38f1f16b7aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-12-11 10:53:16--  https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.13/3.2.1/spark-hadoop-cloud_2.13-3.2.1.jar\n",
      "Resolving repo1.maven.org (repo1.maven.org)... 146.75.92.209, 2a04:4e42:87::209\n",
      "Connecting to repo1.maven.org (repo1.maven.org)|146.75.92.209|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 602385 (588K) [application/java-archive]\n",
      "Saving to: ‘spark-hadoop-cloud_2.13-3.2.1.jar’\n",
      "\n",
      "spark-hadoop-cloud_ 100%[===================>] 588.27K  1.21MB/s    in 0.5s    \n",
      "\n",
      "2024-12-11 10:53:17 (1.21 MB/s) - ‘spark-hadoop-cloud_2.13-3.2.1.jar’ saved [602385/602385]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# !wget https://repo1.maven.org/maven2/org/apache/hive/hive-metastore/2.3.9/hive-metastore-2.3.9-sources.jar\n",
    "\n",
    "# !wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.4_2.12/1.5.0/iceberg-spark-runtime-3.4_2.12-1.5.0.jar\n",
    "# !wget https://repo1.maven.org/maven2/org/projectnessie/nessie-integrations/nessie-spark-extensions-3.4_2.12/0.77.0/nessie-spark-extensions-3.4_2.12-0.77.0.jar\n",
    "# !wget https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.24.8/bundle-2.24.8.jar\n",
    "# !wget https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-nessie/1.4.0/iceberg-nessie-1.4.0.jar\n",
    "# !wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar\n",
    "# !wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/3.3.4/hadoop-common-3.3.4.jar\n",
    "# !wget https://repo1.maven.org/maven2/io/minio/minio/8.5.0/minio-8.5.0.jar\n",
    "# !wget https://repo1.maven.org/maven2/software/amazon/awssdk/url-connection-client/2.24.8/url-connection-client-2.24.8.jar\n",
    "# !wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.207/aws-java-sdk-bundle-1.12.207.jar\n",
    "!wget https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.13/3.2.1/spark-hadoop-cloud_2.13-3.2.1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "a5c911eb-9906-4f77-bd67-bd73bd846ee1"
   },
   "outputs": [],
   "source": [
    "aws_access_key = \"BKky9mVTl8KoPVCmZZ78\"\n",
    "aws_secret_key = \"ZszY0hLzFcJscU8fwAg1bLYSMuWHwpyDpWwfhtmA\"\n",
    "aws_s3_endpoint = \"http://18.140.117.93:9000\" \n",
    "aws_s3_region = \"us-east-1\" \n",
    "nessi_warehouse = \"s3a://standardize\"\n",
    "nessi_uri = \"http://18.140.117.93:19120/api/v1\"\n",
    "raw_csv_path = \"s3a://raw/CRMUSER_ACCOUNTS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5aa6696f-c41a-48ca-8ea3-0f5ab1023d7b"
   },
   "outputs": [],
   "source": [
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('raw_to_standard_etl')\n",
    "        .set('spark.jars.packages', \n",
    "             'org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.5.0,'\n",
    "                'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.4_2.12:0.77.0,' \n",
    "                'software.amazon.awssdk:bundle:2.24.8,' \n",
    "                'org.apache.iceberg:iceberg-nessie:1.4.0,' \n",
    "                'org.apache.hadoop:hadoop-aws:3.3.4,'  \n",
    "                'org.apache.hadoop:hadoop-common:3.3.4,'  \n",
    "                'io.minio:minio:8.5.0,' \n",
    "                'org.apache.spark:spark-hadoop-cloud_2.13:3.2.1,' \n",
    "                'software.amazon.awssdk:url-connection-client:2.24.8,' \n",
    "                'com.amazonaws:aws-java-sdk-bundle:1.12.207')\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', nessi_uri)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', aws_s3_endpoint)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', nessi_warehouse)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        .set('spark.hadoop.fs.s3a.access.key', aws_access_key)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', aws_secret_key)\n",
    "        .set('spark.hadoop.fs.s3a.endpoint', aws_s3_endpoint)\n",
    "        .set('spark.hadoop.fs.s3a.region', aws_s3_region)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "        .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "        .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider')\n",
    "        .set('spark.hadoop.fs.s3a.connection.ssl.enabled', 'false')\n",
    "        .set('spark.hadoop.fs.s3a.aws.credentials.provider', \n",
    "             'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider,'\n",
    "             'org.apache.hadoop.fs.s3a.EnvironmentVariableCredentialsProvider')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1b6ab4fc-4f64-4200-bb4a-58507473c399"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "f56a34dd-cc4f-43b2-b73d-28e360d2ae12"
   },
   "outputs": [],
   "source": [
    "# Set Hadoop Configuration Explicitly\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", aws_s3_endpoint)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b96fc801-9c8f-4eab-ac15-f87e0c2b3265",
    "msg_id": "c8871ec4-33e5-4ee2-a5b8-fc709967a783"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# # Set environment variables before creating Spark Session\n",
    "# os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key\n",
    "# os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_key\n",
    "# os.environ['AWS_S3_ENDPOINT'] = aws_s3_endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6e3065ad-29cb-4d85-8f66-c60557ac9b89"
   },
   "outputs": [],
   "source": [
    "# # Verify Credentials and Connection\n",
    "# def test_s3_connection():\n",
    "#     try:\n",
    "#         # Attempt to list objects in the bucket\n",
    "#         objects = spark.read.csv(f\"s3a://raw/\", header=True)\n",
    "#         print(\"Connection successful!\")\n",
    "#         return True\n",
    "#     except Exception as e:\n",
    "#         print(f\"Connection failed: {e}\")\n",
    "#         return False\n",
    "\n",
    "# Read CSV with explicit options\n",
    "raw_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"s3a://raw/CRMUSER_ACCOUNTS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "38e91348-1864-4175-8774-9913da73e14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+------------+---------+-------+--------------+------+---------+------+--------------+-----------------------+\n",
      "|ACCOUNTID|BANK_ID|CORE_CUST_ID|   ORGKEY|ORGTYPE|          NAME|GENDER|CUST_TYPE|STATUS|PRIMARY_SOL_ID|RELATIONSHIPOPENINGDATE|\n",
      "+---------+-------+------------+---------+-------+--------------+------+---------+------+--------------+-----------------------+\n",
      "|        1|      1|   113391407|113391407|Account|TRAN T *** HAI|     M|    10001| ACTVE|          1404|             23-02-2018|\n",
      "|        2|      1|   113391457|113391457|Account|TRAN T *** YEN|     F|    10001| ACTVE|          1003|              3/12/2018|\n",
      "|        3|      1|   113391483|113391483|Account|TRAN T *** BAC|     F|    10001| ACTVE|          2201|             15-06-2018|\n",
      "|        4|      1|   105260800|105260800|Account|NGUYEN *** UNG|     M|    10001| ACTVE|          1401|             16-11-2018|\n",
      "|        5|      1|   106573010|106573010|Account|TRAN N *** ANH|     M|    10001| ACTVE|          1600|              7/10/2002|\n",
      "|        6|      1|   106573034|106573034|Account|NGUYEN *** ANH|     F|    10001| ACTVE|          1003|              7/10/2002|\n",
      "|        7|      1|   106573046|106573046|Account|PHAN G ***  TU|     M|    10001| ACTVE|          1003|              7/10/2002|\n",
      "|        8|      1|   106573058|106573058|Account|LE THI *** OAN|     F|    10001| ACTVE|          1003|              7/10/2002|\n",
      "|        9|      1|   106573072|106573072|Account|LE KIM *** OAN|     F|    10001| ACTVE|          1003|              7/10/2002|\n",
      "|       10|      1|   118013294|118013294|Account|CHIU T *** UAN|     F|    10001| ACTVE|          1402|              7/10/2002|\n",
      "|       11|      1|   119967428|119967428|Account|NGUYEN *** UAN|     F|    10001| ACTVE|          1601|             14-01-2019|\n",
      "|       12|      1|   106573084|106573084|Account|NGUYEN *** THU|     F|    10001| ACTVE|          1700|             21-01-2019|\n",
      "|       13|      1|   105279368|105279368|Account|CAO HO *** ONG|     M|    10001| ACTVE|          1502|             20-02-2019|\n",
      "|       14|      1|   105279370|105279370|Account|NGUYEN *** YEN|     M|    10001| ACTVE|          1003|             17-06-2019|\n",
      "|       15|      1|   105279435|105279435|Account|LE ANH ***  TU|     M|    10001| ACTVE|          1003|              10/2/2017|\n",
      "|       16|      1|   105268371|105268371|Account|NGO TH *** ANH|     F|    10001| ACTVE|          1015|             25-08-2017|\n",
      "|       17|      1|   105268383|105268383|Account|HUYNH  *** HUY|     F|    10001| ACTVE|          1401|               7/3/2018|\n",
      "|       18|      1|   105268400|105268400|Account|PHAM T *** ANG|     F|    10001| ACTVE|          2100|             14-03-2018|\n",
      "|       19|      1|   119821325|119821325|Account|NGUYEN *** ANG|     F|    10001| ACTVE|          1602|             16-03-2018|\n",
      "|       20|      1|   119821404|119821404|Account|NGUYEN *** OAI|     F|    10001| ACTVE|          1602|             23-03-2018|\n",
      "+---------+-------+------------+---------+-------+--------------+------+---------+------+--------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 with Spark",
   "language": "python3",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
